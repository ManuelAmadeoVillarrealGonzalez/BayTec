#testing connection with database

from datetime import datetime
from time import sleep
from gtts import gTTS
from playsound import playsound
import math
import time
from interbotix_xs_modules.locobot import InterbotixLocobotXS
from subPastillero import *
import paho.mqtt.client as paho

##face recognition modules

import cv2
import numpy as np

from cv2_acquire_visualize_module import *
from face_recognition_module import *

count = 0


# This script uses the perception pipeline to pick up objects and place them in some virtual basket on the left side of the robot
# It also uses the AR tag on the arm to get a better idea of where the arm is relative to the camera (though the URDF is pretty accurate already).
#
# To get started, open a terminal and type...
#'roslaunch interbotix_xslocobot_nav xslocobot_nav.launch robot_model:=locobot_wx250s use_lidar:=true localization:=true use_nav:=true robot_name:=locobot'
# 'roslaunch interbotix_xslocobot_control xslocobot_python.launch robot_model:=locobot_wx250s use_perception:=true use_armtag:=true use_static_transform_pub:=true use_nav:=true localization:=true'
# include robot name in roslaunch if using base 

def backRoom1(bot):
    
    bot.base.move(0,math.pi/4,1,True)
    bot.base.move(0.2,0,1,True)

def backRoom2(bot):
    bot.base.move(0,math.pi/4,1,True)
    bot.base.move(0.2,0,1,True)

def face_recognition():
 
    video_capture = init_camera()

    known_faces_encodings, known_faces_names = load_known_faces_and_encode(KNOWN_FACES_DIR)

    lastPublication = 0.0
    PUBLISH_TIME   = 5     # seconds
    playsound('Audios de interacción/ReconocimientoFacial.mp3')
    while (True):
    #############################################################
    # SENSING LAYER
        rgb_frame, scaled_rgb_frame = acquire_image(video_capture)
    # Percepts are obtained
        face_locations, face_encodings = extract_faces_and_encode(scaled_rgb_frame)
    # Recognition is performed - Short-Long term memory
        face_names = find_face_matches(face_encodings, known_faces_encodings, known_faces_names)
    #############################################################
 
        if np.abs(time.time()-lastPublication) > PUBLISH_TIME:
            try:
                print(face_names)
            except (keyboardInterrupt):
                break
            except Exception as e:
                print(e)
            lastPublication = time.time()
        
        if face_names == ['Unknown'] : count = +1
        if face_names != [] and face_names != ['Unknown'] : break #Se agrega esta condicion para que en cuanto detecte a alguien conocido se cierre y proceda
        if cv2.waitKey(1) & 0xFF == ord('q'):
        # END OF THE GAME/LIFE
            break

    video_capture.release()
    cv2.destroyAllWindows()
    
    return face_names
    

def on_publish(client, userdata, mid):
    print("mid: "+str(mid))
    
def takeVitalSigns(bot,paciente):
    client = paho.Client()
    client.on_publish = on_publish
    client.connect("broker.mqttdashboard.com", 1883)
    client.loop_start()   
    #agregar sensores
    print("Taking vital signs")
    bot.camera.pan_tilt_move(0, -0.9)    
    bot.arm.set_ee_pose_components(x=0.2, z=0.6, pitch=-math.pi/6.0, moving_time=2)
    #playsound('Audios de interacción/TomaTemperatura.mp3')#audio temperatura
    bot.arm.set_ee_cartesian_trajectory(roll=-math.pi/2, moving_time=1)
    playsound('Audios de interacción/TomaTemperatura.mp3') #audio temperatura
    (rc, mid) = client.publish("baytec/bandera", str(paciente), qos=1)
    sleep(15)
    playsound('Audios de interacción/CambioDeSensor.mp3') #adio del seguiente sensor
    bot.arm.set_ee_cartesian_trajectory(roll=math.pi, moving_time=1)
    playsound('Audios de interacción/TomaBMPyOxi.mp3')
    sleep(30)   
    #(rc, mid) = client.publish("baytec/bandera", str("2"), qos=1)

    
def roomTwo(bot,patient):
    playsound('Audios de interacción/Traslado.mp3')
    bot.base.move_to_pose(-1.55,2.2,3.14*1.05,True)
    #bot.base.move(0,math.pi/2,2.7)
    bot.base.move(0.3,0,5.0)
    bot.base.move(0,-math.pi/4,1.4)
    givePills(bot,patient)
    bot.base.move(0,-math.pi/4,4.3)
    bot.base.move(0.3,0,4.5)
    bot.base.move_to_pose(-1.5,0.75,-math.pi/2,True)
    #bot.base.move(0,-math.pi/2,1.42)
    #backRoom2(bot)

def givePills(bot,patient):
    #audio_ReconocimientoFacial.save('ReconocimientoFacial.mp3')
    #añadir face recognition en lugar de "Laura"
    #bot.camera.pan_tilt_move(0, -0.9)   
    #bot.arm.set_ee_pose_components(x=0.2, z=0.6, pitch=-math.pi/6.0, moving_time=2.5)
    #bot.arm.go_to_sleep_pose()
    playsound('Audios de interacción/Saludo.mp3')
    playsound('Audios de interacción/Wait.mp3')
    face_names= face_recognition()
    
    
    if face_names == ['Alonso'] and patient == "Alonso": 
      print("Hola ALonso") 
      #playsound('Audios de interacción/Saludo.mp3')
      playsound('Audios de interacción/Paciente4.mp3')
      #print(pastilleroAlo)
      playsound('Audios de interacción/EntregaPastillas.mp3') # Audio de pastillas
      bot.camera.pan_tilt_move(0, -0.9)   
      bot.arm.set_ee_pose_components(x=0.2, z=0.6, pitch=-math.pi/6.0, moving_time=2.5)
      bot.gripper.open()
      takeVitalSigns(bot,patient)
      playsound('Audios de interacción/Despedida.mp3') #audio de agradecimiento
    elif face_names == ['Amadeo'] and patient == "Amadeo": 
      print("Hola Amadeo") 
      #playsound('Audios de interacción/Saludo.mp3')
      playsound('Audios de interacción/Paciente3.mp3')
      #print(pastilleroAlo)
      playsound('Audios de interacción/EntregaPastillas.mp3') # Audio de pastillas
      bot.camera.pan_tilt_move(0, -0.9)   
      bot.arm.set_ee_pose_components(x=0.2, z=0.6, pitch=-math.pi/6.0, moving_time=2.5)
      bot.gripper.open()
      takeVitalSigns(bot,patient)
      playsound('Audios de interacción/Despedida.mp3') #audio de agradecimiento
    elif face_names == ['Anthony'] and patient == "Anthony": 
      print("Hola Amadeo") 
      #playsound('Audios de interacción/Saludo.mp3')
      playsound('Audios de interacción/Paciente2.mp3')
      #print(pastilleroAlo)
      playsound('Audios de interacción/EntregaPastillas.mp3') # Audio de pastillas
      bot.camera.pan_tilt_move(0, -0.9)   
      bot.arm.set_ee_pose_components(x=0.2, z=0.6, pitch=-math.pi/6.0, moving_time=2.5)
      bot.gripper.open()
      takeVitalSigns(bot,patient)
      playsound('Audios de interacción/Despedida.mp3') #audio de agradecimiento
    elif face_names == ['Laura'] and patient == "Laura": 
      print("Hola Amadeo") 
      #playsound('Audios de interacción/Saludo.mp3')
      playsound('Audios de interacción/Paciente1.mp3')
      #print(pastilleroAlo)
      playsound('Audios de interacción/EntregaPastillas.mp3') # Audio de pastillas
      bot.camera.pan_tilt_move(0, -0.9)   
      bot.arm.set_ee_pose_components(x=0.2, z=0.6, pitch=-math.pi/6.0, moving_time=2.5)
      bot.gripper.open()
      takeVitalSigns(bot,patient)
      playsound('Audios de interacción/Despedida.mp3') #audio de agradecimiento
    elif count > 3:
      print("Unknown patient")
      playsound('Audios de interacción/PacienteNoEncontrado.mp3')

    else:
      print("Wrong patient")
      playsound('Audios de interacción/PacienteNoEncontrado.mp3')
        
    print("Done attending patient ", patient," returning to table")
    bot.arm.set_ee_pose_components(x=0.2, z=0.2)
    bot.arm.go_to_sleep_pose()
    bot.camera.pan_tilt_move(0, 0.3)

   

def roomOne(locobot,patient):
    playsound('Audios de interacción/Traslado.mp3')
    locobot.base.move_to_pose(-1.4,2.2,0.087,True)
    #turning to enter room
    locobot.base.move(0.3,0,3.7)
    locobot.base.move(0,math.pi/2,0.7)
    givePills(locobot,patient)
    locobot.base.move(0,math.pi/2,2.2)
    locobot.base.move(0.3,0,3.0)
    #table position
    locobot.base.move_to_pose(-1.5,0.75,-math.pi/2,True)
    #locobot.base.move(0,-math.pi/2,1.42)
    #backRoom1(locobot)

def pickPills(pillNumber,bot,patient,room):
    count = 0
    bot.base.move_to_pose(-1.5,0.75,-math.pi/2,True)

    okClusters=[] 
   
    print("Picking pill")

    if (pillNumber== "1" or pillNumber == "3"):
    	clusterNumber = 0
    elif (pillNumber == "2" or pillNumber == "4"):
    	clusterNumber = 1
    bot.arm.go_to_sleep_pose()
    time.sleep(1)
    bot.base.move(0.3,0,0.4)
    # move camera such that it's tilting down
    bot.camera.pan_tilt_move(0, 0.6)
    # position the arm such that the Apriltag is clearly visible to the camera
    bot.arm.set_ee_pose_components(x=0.2, z=0.3, pitch=-math.pi/8.0)
    # sleep half a second to give time for the arm to settle after moving
    time.sleep(0.5)
    # get the transform of the AR tag relative to the camera frame; based on that transform,
    # publish a new transform from the 'locobot/plate_link' to the 'locobot/arm_base_link'
    bot.armtag.find_ref_to_arm_base_transform(position_only=True)
    # move the arm out of the way of the camera
    bot.arm.go_to_sleep_pose()
    # get the positions of any clusters present w.r.t. the 'locobot/arm_base_link';
    # sort the clusters such that they appear from left-to-right w.r.t. the 'locobot/arm_base_link'
    time.sleep(0.5)
    #bot.pcl.set_z_filter_min(0.3)
    #bot.pcl.set_cluster_max_size(400)
    bot.camera.pan_tilt_move(0,0.4)
    time.sleep(0.5)
    success, clusters = bot.pcl.get_cluster_positions(ref_frame="locobot/arm_base_link", sort_axis="y", num_samples=5, reverse=True)
    # move the robot back so it's centered and open the gripper
    bot.arm.set_ee_pose_components(x=0.2, z=0.4, moving_time=1.5)
    bot.gripper.open()
    playsound('Audios de interacción/RecogePastillas.mp3')
   
    # pick up each object from left-to-right and drop it in a virtual basket on the left side of the robot
    for cluster in clusters:
        print(cluster)
        x, y, z = cluster["position"]
        print("Z is:",z)
        if (z>=0.27): 
            okClusters.append(cluster)
            
    for cluster in okClusters:
        print(cluster)
        print(okClusters.index(cluster))
        if (okClusters.index(cluster)== clusterNumber):
            x, y, z = cluster["position"]
            print("Cluster position:", cluster["position"])
            bot.arm.set_ee_pose_components(x=x-0.002, y=y+0.01, z=z+0.05,moving_time=2.0)
            bot.arm.set_ee_pose_components(x=x-0.002, y=y+0.01, z=z-0.025)
            bot.gripper.close()
            bot.arm.set_ee_pose_components(x=x, y=y, z=z+0.125,moving_time=2.0)
            bot.base.move(-0.3,0,0.5)
            bot.arm.set_ee_pose_components(x=0.25, z=z)
            #bot.arm.set_ee_cartesian_trajectory(z=-0.2)
            #bot.arm.set_ee_pose_components(x=x, z=0.2)
            #bot.gripper.open()
    bot.arm.set_ee_pose_components(x=0.2,z=0.2,moving_time =2.0)
    bot.camera.pan_tilt_move(0,0.2)
    bot.base.move(-0.3,0,1.0)
    if (room == "1"):
        roomOne(bot,patient)
    elif(room == "2"):
        roomTwo(bot,patient)

def main():

    #audio de inicio de rutina
    playsound('Audios de interacción/Inicio.mp3')
    bot = InterbotixLocobotXS(robot_model="locobot_wx250s", arm_model="mobile_wx250s", use_move_base_action=True)
    sleep(10)
    
    #posicion inicial brazo y gripper
    bot.gripper.open()
    bot.arm.go_to_sleep_pose()
    
    #posicion inicial camara y robot
    bot.camera.pan_tilt_move(0,0.3)
    bot.base.move_to_pose(-1.5,0.75,-math.pi/2,True)
    #bot.base.move(0,math.pi/4,0.3) 
    
    #flag=0
    while(True):
      bot.gripper.open()
      bot.arm.go_to_sleep_pose()      
      #bot.base.move_to_pose(-1.5,0.75,-3.14/2,True)
      #Aqui jalamos el codigo en donde se jala la info de la base de datos
      pastilleroLau,habitacionLau,horarioLau = pastilleroLaura()
      pastilleroAnth,habitacionAnth,horarioAnth = pastilleroAnthony()
      pastilleroAmad,habitacionAmad,horarioAmad = pastilleroAmadeo()
      pastilleroAlo,habitacionAlo,horarioAlo = pastilleroAlonso()
      #Jalamos la hora actual y la establecemos en formato hora:minuto
      now = datetime.now()
      tiempo = now.strftime("%H:%M")
      print(tiempo) # Impirmimos la hora actual
  
      
      #Recorremos la lista e imprimimos horario por horario dentro de la lista
      print("Horario Lau")
      for horario in horarioLau:
        print(horario)
        if horario == tiempo: 
          print("A darle Lau") #Cuando la hora real coincida con una de las horas de la lista esto se ejecuta
          #ir del locobot
          print(habitacionLau)

          pickPills(pastilleroLau,bot,"Laura",habitacionLau)
          #from BayTec.cv2_face_recognition import * #funcion face
      print("Horario Anthony") 
      for horario in horarioAnth:
        print(horario)
        if horario == tiempo: 
          print("A darle Anthony") #Cuando la hora real coincida con una de las horas de la lista esto se ejecuta
          #ir del locobot
          print(habitacionAnth)

          pickPills(pastilleroAnth,bot,"Anthony",habitacionAnth)
          #from BayTec.cv2_face_recognition import * #funcion face
      print("Horario Amadeo")    
      for horario in horarioAmad:
        print(horario)
        if horario == tiempo: 
          print("A darle Amadeo") #Cuando la hora real coincida con una de las horas de la lista esto se ejecuta
          #ir del locobot
          print(habitacionAmad)

          pickPills(pastilleroAmad,bot,"Amadeo",habitacionAmad)
          #from BayTec.cv2_face_recognition import * #funcion face
      print("Horario Alonso")
      for horario in horarioAlo:
        print(horario)
        if horario == tiempo: 
          print("A darle Alonso") #Cuando la hora real coincida con una de las horas de la lista esto se ejecuta   
          #ir del locobot
          print(habitacionAlo)

          pickPills(pastilleroAlo,bot,"Alonso",habitacionAlo)
          #from cv2_face_recognition import * #funcion face    

    #pickPills("1",bot,"Alonso","1")
    
    
                
if __name__=='__main__':
    main()
